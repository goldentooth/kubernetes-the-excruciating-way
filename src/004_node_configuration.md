# Node Configuration

After physically installing and setting up the nodes, the next step is to perform basic configuration. You can see the [Ansible playbook](https://github.com/goldentooth/cluster/blob/main/playbooks/configure_cluster.yaml) I use for this, which currently runs the following roles:

- [`goldentooth.configure`](https://github.com/goldentooth/cluster/blob/main/roles/goldentooth.configure/tasks/main.yaml):
  - Set timezone; last thing I need to do when working with computers is having to perform arithmetic on times and dates.
  - Set keybord layout; this should be set already, but I want to be sure.
  - Enable overclocking; I've installed an adequate cooling system to support the Pis running full-throttle at their full spec clock.
  - Enable fan control; the heatsinks I've installed include fans to prevent CPU throttling under heavy load.
  - Enable and configure certain cgroups; this allows Kubernetes to manage and limit resources on the system.
    - `cpuset`: This is used to manage the assignment of individual CPUs (both physical and logical) and memory nodes to tasks running in a cgroup. It allows for pinning processes to specific CPUs and memory nodes, which can be very useful in a containerized environment for performance tuning and ensuring that certain processes have dedicated CPU time. Kubernetes can use cpuset to ensure that workloads (containers/pods) have dedicated processing resources. This is particularly important in multi-tenant environments or when running workloads that require guaranteed CPU cycles. By controlling CPU affinity and ensuring that processes are not competing for CPU time, Kubernetes can improve the predictability and efficiency of applications.
    - `memory`: This is used to limit the amount of memory that tasks in a cgroup can use. This includes both RAM and swap space. It provides mechanisms to monitor memory usage and enforce hard or soft limits on the memory available to processes. When a limit is reached, the cgroup can trigger OOM (Out of Memory) killer to select and kill processes exceeding their allocation. Kubernetes uses the memory cgroup to enforce memory limits specified for pods and containers, preventing a single workload from consuming all available memory, which could lead to system instability or affect other workloads. It allows for better resource isolation, efficient use of system resources, and ensures that applications adhere to their specified resource limits, promoting fairness and reliability.
    - `hugetlb`: This is used to manage huge pages, a feature of modern operating systems that allows the allocation of memory in larger blocks (huge pages) compared to standard page sizes. This can significantly improve performance for certain workloads by reducing the overhead of page translation and increasing TLB (Translation Lookaside Buffer) hits. Some applications, particularly those dealing with large datasets or high-performance computing tasks, can benefit significantly from using huge pages. Kubernetes can use it to allocate huge pages to these workloads, improving performance and efficiency. This is not going to be a concern for my use, but I'm enabling it anyway simply because it's recommended.
  - Disable swap. Kubernetes doesn't like swap by default, and although this can be worked around, I'd prefer to avoid swapping on SD cards. I don't really expect a high memory pressure condition anyway.
  - Set preferred editor; I like `nano`, although I can (after years of practice) safely and reliably exit `vi`.
  - Set certain kernel modules to load at boot:
    - `overlay`: This supports OverlayFS, a type of union filesystem. It allows one filesystem to be overlaid on top of another, combining their contents. In the context of containers, OverlayFS can be used to create a layered filesystem that combines multiple layers into a single view, making it efficient to manage container images and writable container layers.
    - `br_netfilter`: This allows bridged network traffic to be filtered by iptables and ip6tables. This is essential for implementing network policies, including those related to Network Address Translation (NAT), port forwarding, and traffic filtering. Kubernetes uses it to enforce network policies that control ingress and egress traffic to pods and between pods. This is crucial for maintaining the security and isolation of containerized applications. It also enables the necessary manipulation of traffic for services to direct traffic to pods, and for pods to communicate with each other and the outside world. This includes the implementation of services, load balancing, and NAT for pod networking.
    And by allowing iptables to filter bridged traffic, br_netfilter helps Kubernetes manage network traffic more efficiently, ensuring consistent network performance and reliability across the cluster.
  - Load above kernel modules on every boot.
  - Set some kernel parameters:
    - `net.bridge.bridge-nf-call-iptables`: This allows iptables to inspect and manipulate the traffic that passes through a Linux bridge. A bridge is a way to connect two network segments, acting somewhat like a virtual network switch. When enabled, it allows iptables rules to be applied to traffic coming in or going out of a bridge, effectively enabling network policies, NAT, and other iptables-based functionalities for bridged traffic. This is essential in Kubernetes for implementing network policies that control access to and from pods running on the same node, ensuring the necessary level of network isolation and security.
    - `net.bridge.bridge-nf-call-ip6tables`: As above, but for IPv6 traffic.
    - `net.ipv4.ip_forward`: This controls the ability of the Linux kernel to forward IP packets from one network interface to another, a fundamental capability for any router or gateway. Enabling IP forwarding is crucial for a node to route traffic between pods, across different nodes, or between pods and the external network. It allows the node to act as a forwarder or router, which is essential for the connectivity of pods across the cluster, service exposure, and for pods to access the internet or external resources when necessary.
  - Add SSH public key to `root`'s authorized keys; this is already performed for my normal user by Raspberry Pi Imager.
- [`goldentooth.set_hostname`](https://github.com/goldentooth/cluster/blob/main/roles/goldentooth.set_hostname/tasks/main.yaml): Set the hostname of the node (including a line in `/etc/hosts`). This doesn't need to be a separate role, obviously. I just like the structure as I have it.
- [`goldentooth.set_motd`](https://github.com/goldentooth/cluster/blob/main/roles/goldentooth.set_motd/tasks/main.yaml): Set the MotD, as described in the previous chapter.
- [`goldentooth.set_bash_prompt`](https://github.com/goldentooth/cluster/blob/main/roles/goldentooth.set_bash_prompt/tasks/main.yaml): Set the Bash prompt, as described in the previous chapter.
- [`goldentooth.setup_security`](https://github.com/goldentooth/cluster/blob/main/roles/goldentooth.setup_security/tasks/main.yaml): Some basic security configuration. Currently, this just uses Jeff Geerling's `ansible-role-security` to perform some basic tasks, like setting up unattended upgrades, etc, but I might expand this in the future.

Raspberry Pi Imager doesn't allow you to specify an SSH key for the `root` user, so I do this in `goldentooth.configure`. However, I also have Kubespray installed (for when I want things to Just Workâ„¢), and Kubespray expects the remote user to be `root`. As a result, I specify that the remote user is my normal user account in the `configure_cluster` playbook. This means a lot of `become: true` in the roles, but I would prefer eventually to ditch Kubespray and disallow root login via SSH.

Anyway, we need to rerun `goldentooth.set_bash_prompt`, but as the `root` user. This almost never matters, since I prefer to SSH as a normal user and use `sudo`, but I like my prompts and you can't take them away from me.

With the nodes configured, we can start talking about the different roles they will serve.
